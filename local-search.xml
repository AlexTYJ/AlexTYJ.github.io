<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>data2vec 论文阅读</title>
    <link href="/2025/03/10/data2vec-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/03/10/data2vec-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>和Bert等预训练模型的本质区别在于：Bert在原始文本上mask，而该模型在latentrepresentation上进行预测。文章认为所有模态在latentrepresentation上都是一些稠密向量，所以训练的方法相同。</p><h1 id="模型细节">模型细节</h1><p>所有模态的数据在encoder之后。然后复制两份，一份mask之后通过studentmodel，只在mask的地方产生输出。一份不mask，给teachermodel直接学习上下文表示，然后让两者的输出对齐。</p><h2 id="encoder-细节">Encoder 细节</h2><p>encoder部分并没有将不同模态统一。</p><ul><li>视觉：参考ViT</li><li>语音： 一个多层 1-D 卷积层， 16 kHz waveform to 50 Hzrepresentations</li><li>文本：token embedding</li></ul><h2 id="masking-细节">Masking 细节</h2><p>mask也不是统一的。</p><ul><li>视觉：block-wise masking strategy</li><li>语音：mask spans of latent speech representations</li><li>文本：tokens</li></ul><h2 id="student-model-teacher-model细节">student model &amp; teachermodel细节</h2><p>都是普通Transformer。</p><p>其中，teacher model的参数遵循以下公式：</p><p><span class="math display">\[\Delta \leftarrow \tau \Delta + (1 - \tau) \theta\]</span></p><p>其中 <span class="math inline">\(\Delta\)</span> 是teachermodel的参数， <span class="math inline">\(\theta\)</span> 是studentmodel 的参数。 <span class="math inline">\(\tau\)</span>是缓慢增加的。</p><p>学生模型的训练目标是：</p><p><span class="math display">\[y_t = \frac{1}{K} \sum_{l=L-K+1}^{L} \hat{a}_{l,t}\]</span></p><p>其中， <span class="math inline">\(\hat{a}_{l,t}\)</span> 是teachermodel经过归一化的第<span class="math inline">\(l\)</span>个块在时间步<span class="math inline">\(t\)</span> 的输出，<spanclass="math inline">\(K\)</span>是前<spanclass="math inline">\(K\)</span>个块的数量，<spanclass="math inline">\(L\)</span>是总块数。</p>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN-T 论文阅读</title>
    <link href="/2025/03/05/RNN-T-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/03/05/RNN-T-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="公式推导">公式推导</h1><p>定义：</p><ul><li>定义任务输入：<span class="math inline">\(x = (x_1, x_2, \dots,x_T)\)</span>，其中 <span class="math inline">\(x\)</span> 中的元素<span class="math inline">\(x_i \in \mathcal{X}\)</span>，<spanclass="math inline">\(x \in \mathcal{X}^*\)</span>。每一个<spanclass="math inline">\(x_i\)</span> 一般为MFCC的倒谱系数构成的向量。</li><li>定义任务输出：<span class="math inline">\(y = (y_1, y_2, \dots,y_U)\)</span>，其中 <span class="math inline">\(y\)</span> 中的元素<span class="math inline">\(y_i \in \mathcal{Y}\)</span>，<spanclass="math inline">\(y \in \mathcal{Y}^*\)</span>。每一个 <spanclass="math inline">\(y_i\)</span>一般是一个one-hot的向量，编码特定的phoneme。</li><li>定义集合 <span class="math inline">\(\bar{\mathcal{Y}} = \mathcal{Y}\cup \{\varnothing\}\)</span>，作用是在输出中加入一些<spanclass="math inline">\(\varnothing\)</span></li><li>定义函数 <span class="math inline">\(B: \bar{\mathcal{Y}}^* \to\mathcal{Y}^*\)</span>，作用是去除所有的 <spanclass="math inline">\(\varnothing\)</span></li><li>定义模型RNN-Transducer，输入是<spanclass="math inline">\(x\)</span>，输出是<span class="math inline">\(y\in\mathcal{Y}^*\)</span>，该模型的概率建模为：<spanclass="math inline">\(\Pr(y \in \mathcal{Y}^* | x)\)</span></li></ul><p>由以上定义我们可以得知： <span class="math display">\[\Pr(y \in\mathcal{Y}^* | x)=\sum_{a \in B^{-1}(y)} \Pr(a | x)\]</span>即我们需要枚举所有可能的添加 <spanclass="math inline">\(\varnothing\)</span>的方式，把它们的概率相加，得到最终的概率。</p><p>下面开始，我们来想办法解决<spanclass="math inline">\(\sum\)</span>内的部分，即 <spanclass="math inline">\(\Pr(a \in \bar {\mathcal{Y}}^* | x)\)</span></p><p>我们使用两个网络：</p><ol type="1"><li>Transcription Network：输入为 <spanclass="math inline">\(x\)</span>, 输出Transcription Vectors <spanclass="math inline">\(f=(f_1,f_2,...,f_T)\)</span>（为了方便起见定义长度为<spanclass="math inline">\(T\)</span>，但实际上如果做pooling的话可能不是<spanclass="math inline">\(T\)</span></li><li>Prediction Network：输入为 <spanclass="math inline">\(y\)</span>，输出为prediction vector <spanclass="math inline">\(g=(g_0,g_1,...,g_U)\)</span></li></ol><h2 id="prediction-network">Prediction Network</h2><ul><li><p>Prediction Network <span class="math inline">\(G\)</span>是一个RNN或LSTM。</p></li><li><p>目标：在给定过去的输出序列<spanclass="math inline">\(y\)</span>时，预测下一个可能的输出元素。因此，它类似于标准的next-stepprediction RNN（或LSTM），但额外允许预测<spanclass="math inline">\(\varnothing\)</span>。</p></li><li><p>输入：<span class="math inline">\((\varnothing,y_1,y_2,...,y_U)\)</span>，即在<spanclass="math inline">\(y\)</span>前面加了一个<spanclass="math inline">\(\varnothing\)</span>。每个<spanclass="math inline">\(y_i\)</span>采用one-hot编码，假设维度为K。</p></li><li><p>输出：<spanclass="math inline">\(g=(g_0,g_1,...,g_U)\)</span>，每个<spanclass="math inline">\(g_i\)</span>维度为K+1（加上了<spanclass="math inline">\(\varnothing\)</span>）。</p></li></ul><h2 id="transcription-network">Transcription Network</h2><ul><li>Transcription Network <span class="math inline">\(F\)</span>是一个双向RNN（或LSTM）。</li><li>输入为<spanclass="math inline">\(x\)</span>，也就是MFCC的系数向量序列。</li><li>输出为<spanclass="math inline">\(f\)</span>，同样为K+1维向量的序列（加上了<spanclass="math inline">\(\varnothing\)</span>）。</li></ul><h2 id="output-distribution">Output Distribution</h2><p>我们回顾两个网络的输入输出：</p><ol type="1"><li>Transcription Network：输入 <span class="math inline">\(x\)</span>,输出<span class="math inline">\(f=(f_1,f_2,...,f_T)\)</span></li><li>Prediction Network：输入 <spanclass="math inline">\(y\)</span>，输出 <spanclass="math inline">\(g=(g_0,g_1,...,g_U)\)</span></li></ol><p>接下来我们整合两个网络。定义: <spanclass="math display">\[h(k,t,u)=e^{f_t^k+g_u^k}\]</span></p><p>其中<span class="math inline">\(1\leq t \leq T\)</span>, <spanclass="math inline">\(0 \leq u \leq U\)</span>，标签 <spanclass="math inline">\(k\in \bar{\mathcal{Y}}\)</span>。<spanclass="math inline">\(k\)</span>作为上标时表示向量的第<spanclass="math inline">\(k\)</span>个元素。</p><p>归一化： <span class="math display">\[\Pr(k \in \bar {\mathcal{Y}} |t,u)=\frac{h(k,t,u)}{\sum_{k&#39;\in\mathcal{Y}}h(k&#39;,t,u)}\]</span></p><p>这个式子代表着：在<span class="math inline">\(f\)</span>输出到<spanclass="math inline">\(t\)</span>，<spanclass="math inline">\(g\)</span>输出到<spanclass="math inline">\(u\)</span>时，预测为<spanclass="math inline">\(k\)</span>的概率。而<spanclass="math inline">\(f_t^k\)</span>和<spanclass="math inline">\(g_u^k\)</span>可以直接相加的底气在于，他们都代表着同一意义的概率。</p><blockquote><p>Q：为什么要枚举<span class="math inline">\(t\)</span>和<spanclass="math inline">\(u\)</span>呢？他们不应该都等于<spanclass="math inline">\(k\)</span>吗？</p></blockquote><blockquote><p>A：因为存在<spanclass="math inline">\(\varnothing\)</span>，所以在同一个位置上，<spanclass="math inline">\(f\)</span>和<spanclass="math inline">\(g\)</span>并不一定进行到了同一个字符，所以需要枚举所有<spanclass="math inline">\(t\)</span>和<spanclass="math inline">\(u\)</span>。</p></blockquote><blockquote><p>实际上，由于<span class="math inline">\(g\)</span>的输入是<spanclass="math inline">\(y\)</span>，它除了第一个字符是<spanclass="math inline">\(\varnothing\)</span>，后面的都是字符，而<spanclass="math inline">\(f\)</span>却可能出现很多<spanclass="math inline">\(\varnothing\)</span>，因此他们大概率是对不齐的。</p></blockquote><p>现在我们对比我们的目标式子：</p><p><span class="math display">\[\Pr(a \in \bar {\mathcal{Y}}^* |x)\]</span></p><p>我们还剩最后一步：对<span class="math inline">\(t\)</span>和<spanclass="math inline">\(u\)</span>进行累计，求出联合概率分布。</p><p>为了方便推导，我们定义两个记号：</p><p><span class="math display">\[y(t,u) \equiv \Pr(y_{u+1} | t, u)\]</span> <span class="math display">\[\varnothing(t, u) \equiv \Pr(\varnothing | t, u)\]</span> 他们表示在<spanclass="math inline">\((t,u)\)</span>这个节点上，预测出的为答案为字符或者<spanclass="math inline">\(\varnothing\)</span>的概率。</p><h2 id="forward-backward-算法">Forward-Backward 算法</h2><p>这个算法告诉我们</p><h3 id="forward算法">Forward算法</h3><p>定义前向变量 <span class="math inline">\(\alpha(t, u)\)</span> 为在<span class="math inline">\(f[1:t]\)</span> 中输出 <spanclass="math inline">\(y[1:u]\)</span>的概率。所有 <spanclass="math inline">\(1 \leq t \leq T\)</span> 和 <spanclass="math inline">\(0 \leq u \leq U\)</span>的前向变量可以通过递归计算：</p><p><span class="math display">\[\alpha(t, u) = \alpha(t - 1, u) \varnothing(t - 1, u) + \alpha(t, u - 1)y(t, u - 1)  \]</span></p><p>初始条件为：<span class="math inline">\(\alpha(1, 0) = 1\)</span></p><p>这个式子的意思是：在<span class="math inline">\(f[1:t]\)</span>中输出<span class="math inline">\(y[1:u]\)</span>的概率，等于在<spanclass="math inline">\(f[1:t-1]\)</span>中输出 <spanclass="math inline">\(y[1:u]\)</span>的概率乘上在<spanclass="math inline">\((t,u)\)</span>这个位置生成 <spanclass="math inline">\(\varnothing\)</span> 的概率，加上在<spanclass="math inline">\(f[1:t]\)</span>中输出 <spanclass="math inline">\(y[1:u-1]\)</span>的概率乘上在<spanclass="math inline">\((t,u)\)</span>这个位置生成字符的概率。</p><p>可以形象的表示为下图：<img src="/images/RNN-T-forward.png" alt="RNN-T的Forward示例" width="400"></p><p>其中，底部的黑色节点表示在没有输出任何内容之前的空状态。</p><h2 id="backward算法">Backward算法</h2><p>类似的，我们可以定义后向变量 <span class="math inline">\(\beta(t,u)\)</span> 为在 <span class="math inline">\(f[t:T]\)</span> 中输出<span class="math inline">\(y[u+1:U]\)</span> 的概率。</p><p><span class="math display">\[ \beta(t, u) = \beta(t + 1, u)\varnothing(t, u) + \beta(t, u + 1) y(t, u)  \]</span></p><p>初始条件为：<span class="math inline">\(\beta (T,U) = \varnothing(T,U)\)</span></p><h2 id="loss-函数">Loss 函数</h2><p>至此，我们终于可以求出</p><p><span class="math display">\[\Pr(a \in \bar {\mathcal{Y}}^* | x) =\sum_{(t, u): t + u = n} \alpha(t, u) \beta(t, u) \]</span></p><p>也就是左上到右下对角线上<span class="math inline">\(\alpha(t, u)\beta(t, u)\)</span> 的总和。</p><p>于是Loss函数就是： <span class="math display">\[L = - \ln Pr(y^* | x)\]</span></p>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pruned RNN-T 论文阅读</title>
    <link href="/2025/03/05/Pruned-RNN-T-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/03/05/Pruned-RNN-T-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="rnn-t-的缺点">RNN-T 的缺点</h1><ol type="1"><li>损失函数计算相对较慢</li><li>占用较多内存</li></ol><p>RNN-T模型需要储存一个4维的张量，维度是 <spanclass="math inline">\((N, T, U, C)\)</span>, 其中<spanclass="math inline">\(N\)</span>是batch_size，<spanclass="math inline">\(T\)</span>是Transcription Network的输出长度，<spanclass="math inline">\(U\)</span>是Prediction的输出长度，<spanclass="math inline">\(V\)</span>是词表大小。</p><p>本文将<span class="math inline">\(U\)</span>限制为 <spanclass="math inline">\(S\subseteq U\)</span>，从而加速训练。</p><h1 id="基本思想">基本思想</h1><p>仅计算对最终损失有重要贡献的 <spanclass="math inline">\((t,u)\)</span> 对应项的joinernetwork，从而减少计算量。</p><p>具体来说，我们执行两次递推：</p><ol type="1"><li>使用一个trivial的joinernetwork，该网络计算速度非常快。利用初步计算结果：确定哪些索引对损失贡献较大。</li><li>第二次计算：仅对筛选出的 <spanclass="math inline">\((t,u)\)</span>子集来计算的joiner network。</li></ol><h2 id="trivial-joiner-network">Trivial Joiner Network</h2><p>Trivial Joiner Network的思路为：</p><p>直接对encoder embedding和decoderembedding进行投影，得到未归一化的对数概率：</p><p><span class="math display">\[L_{\text{enc}}(t, v)\]</span></p><p><span class="math display">\[L_{\text{dec}}(u, v)\]</span></p><p>那么最终的loss就是：</p><p><span class="math display">\[L_{\text{trivial}}(t, u, v) =L_{\text{enc}}(t, v) + L_{\text{dec}}(u, v) - L_{\text{normalizer}}(t,u)\]</span></p><p>其中</p><p><span class="math display">\[L_{\text{normalizer}}(t, u) = \log \sum_{v} \exp \left(L_{\text{enc}}(t, v) + L_{\text{dec}}(u, v) \right)\]</span></p><p>这个公式本质上是log-space的矩阵乘法，容易实现。</p><h2 id="pruning-bounds">Pruning Bounds</h2><p>引入常数<span class="math inline">\(S\)</span>，表示对于任意的<spanclass="math inline">\(t\)</span>，我们将仅计算考虑 <spanclass="math inline">\(L(t, u, v)\)</span> 在<spanclass="math inline">\(p_t \leq u &lt; p_t + S\)</span> 位置上的值。其中<span class="math inline">\(p_t\)</span> 是通过Trivial JoinerNetwork得到的。</p><p>剪枝就是将 <span class="math inline">\(u &lt; p_t\)</span> 或 <spanclass="math inline">\(u \geq p_t + S\)</span> 的 <spanclass="math inline">\(P(t, u, \cdot)\)</span> 视为 <spanclass="math inline">\(-\infty\)</span>。</p><p>理想情况下，我们希望找到一组整数剪枝边界 <spanclass="math inline">\(p = p_0, p_t, \dots,p_{T-1}\)</span>，该组边界能够最大化Trivial JoinerNetwork的总保留概率。换句话说，我们要最大化数据的似然性。</p><p>为了简化问题，我们通过为每个帧 <span class="math inline">\(t\)</span>找到局部最优的剪枝边界来解决，然后应用一些连续性约束来改进结果。</p><p>具体来说，就是求解下面这个公式：</p><p><span class="math display">\[p_t = \arg\max_{p=0}^{U-S+1} \left( -y&#39;_0(t, p-1) + \sum_{u=p}^{p+S-1} \varnothing&#39;_0(t, u)\right)\]</span></p><p>其中 <span class="math inline">\(y&#39;_0(t, u)\)</span> 和 <spanclass="math inline">\(\varnothing&#39;_0(t, u)\)</span> 是 <spanclass="math inline">\(L_{tot}\)</span> 关于<spanclass="math inline">\(y(t, u)\)</span> 和<spanclass="math inline">\(\varnothing(t, u)\)</span>.的导数。</p><p>这个公式可以形象的理解为下图中绿线减去红线的值。</p><p><img src="/images/pruned-RNN-T.png" alt="Pruned-RNN-T" width="400"></p><p>减去红线的原因是？</p><p>当然还需要满足一些条件（连续性等）：</p><p><span class="math display">\[0 \leq p_t \leq U - S + 1\]</span></p><p><span class="math display">\[p_t \leq p_{t+1}\]</span></p><p><span class="math display">\[p_{t+1} - p_t \leq S\]</span></p><h2 id="loss-function">Loss function</h2><p><span class="math display">\[L_{\text{smoothed}}(t, u, v)\]</span></p><p><span class="math display">\[= (1 - \alpha_{\text{lm}} - \alpha_{\text{acoustic}}) \cdotL_{\text{trivial}}(t, u, v) + \alpha_{\text{lm}} \cdot L_{\text{lm}}(t,u, v) + \alpha_{\text{acoustic}} \cdot L_{\text{acoustic}}(t, u, v)\]</span></p><p>其中：</p><p><span class="math display">\[L_{\text{trivial}}(t, u, v) = \text{LogSoftmax}_v \left(L_{\text{enc}}(t, v) + L_{\text{dec}}(u, v) \right)  \]</span></p><p><span class="math display">\[L_{\text{acoustic}}(t, u, v) = \text{LogSoftmax}_v \left(L_{\text{enc}}(t, v) + L_{\text{avg-dec}}(u, v) \right)  \]</span></p><p><span class="math display">\[L_{\text{lm}}(t, u, v) = \text{LogSoftmax}_v (L_{\text{dec}}(u, v))\]</span></p><p><span class="math display">\[L_{\text{avg-dec}}(u, v) = \log \left( \frac{1}{U + 1} \right)\sum_{u=0}^{X} \text{Softmax}_v L_{\text{dec}}(u, v)  \]</span></p><p>为什么要平均？</p>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BPE 论文阅读</title>
    <link href="/2025/03/03/BPE-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/03/03/BPE-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>为了解决罕见词不在词汇表中的问题。</p><p>每次合并出现频率最高的词组。</p><p>唯一超参数为合并次数</p><h1 id="伪代码">伪代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python">Initialize vocabulary <span class="hljs-keyword">with</span> word frequencies<br><br><span class="hljs-type">Set</span> number of merge operations<br><br>Define function get_stats(vocabulary):<br>    Create empty dictionary <span class="hljs-keyword">for</span> pairs<br>    For each word <span class="hljs-keyword">and</span> its frequency <span class="hljs-keyword">in</span> vocabulary:<br>        Split word into symbols<br>        For each adjacent pair of symbols:<br>            Count occurrences of each pair<br>    Return the dictionary of pairs <span class="hljs-keyword">with</span> their frequencies<br><br>Define function merge_vocab(pair, vocabulary):<br>    Create empty dictionary <span class="hljs-keyword">for</span> new vocabulary<br>    Define pattern <span class="hljs-keyword">as</span> the pair joined by space<br>    Define replacement <span class="hljs-keyword">as</span> the pair joined without space<br>    <br>    For each word <span class="hljs-keyword">and</span> its frequency <span class="hljs-keyword">in</span> vocabulary:<br>        Replace occurrences of the pair <span class="hljs-keyword">in</span> the word <span class="hljs-keyword">with</span> the merged version<br>        Store the new word <span class="hljs-keyword">and</span> its frequency <span class="hljs-keyword">in</span> new vocabulary<br>    <br>    Return new vocabulary<br><br>Repeat <span class="hljs-keyword">for</span> the defined number of merge operations:<br>    Get symbol pair frequencies <span class="hljs-keyword">from</span> vocabulary<br>    If no pairs exist, exit loop<br>    Find the most frequent pair<br>    Merge the most frequent pair <span class="hljs-keyword">in</span> vocabulary<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN-Transducer with stateless prediction network 论文阅读</title>
    <link href="/2025/03/03/RNN-Transducer-with-stateless-prediction-network-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/03/03/RNN-Transducer-with-stateless-prediction-network-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>过去， predictionnetwork被认为是语言模型。这篇文章推翻了这一论点。</p><p>证据是，经过实验，一个stateless(例如非recurrent)的predictionnetwork（这种网络仅依赖于上一个时刻输出）在使用wordpieces时，几乎能够与原始RNNT表现相当。</p><p>结论：RNNT 的 prediction network并不像经典 ASR系统中的语言模型那样工作。相反，它的作用仅限于帮助模型 align输入音频，而 RNNT 的encoder 和 joint networksacoustic和linguistic信息。</p><h1 id="实验">实验</h1><p>三组实验：</p><p>首先用文本数据预训练prediction network</p><ol type="1"><li>不冻结</li><li>冻结prediction network</li><li>prediction network中增加两层，冻结</li></ol><figure><img src="/images/stateless_results_1.png"alt="freezing predictor network的WER最高" /><figcaption aria-hidden="true">freezing predictornetwork的WER最高</figcaption></figure><p>结果：freezing predictornetwork的WER最高。这表明文本数据中学到的知识反而会增加WER。</p><h1 id="stateless-prediction-network-的表现">Stateless predictionnetwork 的表现</h1><figure><img src="/images/RNNT-SLP.png" alt="RNNT-SLP结构图" /><figcaption aria-hidden="true">RNNT-SLP结构图</figcaption></figure><p>stateless prediction network 如果使用graphemes会出现重复的问题。由于只看到前一个单词，所以没有信息知道已经说了几个。例如，food可能会输出fooooood。如果使用wordpiece就不会有这个问题。</p>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>unispeech 论文阅读</title>
    <link href="/2025/02/27/unispeech-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/02/27/unispeech-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>利用L、M、N三个数据集（定义见下）。</p><ol type="1"><li><p>在L、M上预训练模型。</p></li><li><p>冻结特征提取器，并在N上对Transformer部分进行微调。</p></li></ol><h1 id="定义">定义</h1><p>L：大规模、有标签的high-resource语种数据集</p><p>M：大规模、无标签的low-resource语种数据集</p><p>N：小规模、有标签的low-resource语种数据集</p><h1 id="模型结构">模型结构</h1><p><img src="/images/unispeech.png" alt="unispeech" width="600"></p><p>使用multitask学习方法。</p><ul><li>目标1：sequence-level CTC loss</li></ul><p><span class="math display">\[L_{\text{ctc}} = - \log p_{\text{ctc}}(y|x) = \sum_{\pi \in \Phi_{x,y}}p(\pi | c_1, \dots, c_T)\]</span></p><ul><li>目标2：基于wav2vec2.0，在masked contextual representations和discretelatent representations之间的对比学习</li></ul><p>对比学习loss:</p><p><span class="math display">\[L_c = - \log \frac{\exp(\text{sim}(c_t, q_t) / \kappa)}{\sum_{q&#39;\sim Q_t} \exp(\text{sim}(c_t, q&#39;) / \kappa)}\]</span></p><p>codebook diversity loss：</p><p><span class="math display">\[\quad L_d = \frac{1}{GV} \sum_{g=1}^{G} \sum_{v=1}^{V} \bar{p}_{g,v}\log \bar{p}_{g,v}\]</span></p><p><span class="math display">\[L_{\text{self}} = L_c + 0.1 \times L_d\]</span></p><p>最终预训练的loss：</p><p><span class="math display">\[\quad L = \sum_{(x,y) \in L} \left( \alpha L_{\text{ctc}} + (1 - \alpha)L_{\text{self}} \right) + \sum_{(x) \in M} L_{\text{self}}\]</span></p><h1 id="问题">问题</h1><ol type="1"><li>训练时是混合训练吗？</li><li>为什么要训练码本？如果码本和Transformer结构要尽量相似的话</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Factorized RNN-T 论文阅读</title>
    <link href="/2025/02/23/factorized_rnnt-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/02/23/factorized_rnnt-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="为什么-neural-transducer-中的predictor无法被视作语言模型">为什么Neural Transducer 中的Predictor无法被视作语言模型</h1><p>唯一区别在于因为需要预测 φ。因此想到可以将Predictor拆分。</p><h1 id="factorized-neural-transducer">Factorized Neural Transducer</h1><p><img src="/images/factorized_neural_Transducer.png" alt="factorized neural Transducer" width="600"></p><p>把Predictor拆分成两部分。</p><ol type="1"><li>专门用于预测空白符号 φ，称为blank predictor;</li><li>另一个用于预测标签词汇（不包括 φ）， 称为vocabulary predictor</li></ol><p>这样，Predictor_v就相当于一个语言模型。</p><h1 id="loss-function">Loss function</h1><p>原始的Transducer的Loss为：</p><p><span class="math display">\[ J_t = -\log P(y \in Y^* | x) = -\log\sum_{\alpha \in \beta^{-1}(y)} P(\alpha | x) \]</span></p><p>而factorized_rnnt的Loss为：</p><p><span class="math display">\[J_f = J_t - \lambda \log P(y_1^U)\]</span></p>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HuBert论文阅读</title>
    <link href="/2025/02/19/HuBert%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/02/19/HuBert%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<figure><img src="/images/HuBert_1.png" alt="HuBert示意图" /><figcaption aria-hidden="true">HuBert示意图</figcaption></figure><h1 id="概述">概述</h1><ul><li>作用：从无监督数据中学习隐层表示。</li><li>端口：输入为音频，输出为隐层表示。</li></ul><h1 id="训练过程">训练过程：</h1><ol type="1"><li>利用Acoustic Unit Discovery System得到label</li><li>通过HuBERT从mask之后的输入中得到隐层表示z</li><li>计算label和预测之间的loss</li></ol><h1 id="细节">细节</h1><h2 id="acoustic-unit-discovery">Acoustic Unit Discovery</h2><p>可以理解为语音离散化。</p><p>一些记号：</p><p>T个frame的语音 $ X = [x_1,..,x_T] $。Acoustic UnitDiscovery模型输出的hidden units为 $ h(X)=Z=[z_1,..,z_T]$，其中 <spanclass="math inline">\(z_t \in [C]\)</span>是一个C-class的变量。h是一个非监督的聚类模型，如k-means on MFCC。</p><h2 id="mask">Mask</h2><p>使用了和wav2vec 2类似的方法，首先选择 <spanclass="math inline">\(p%\)</span>time-steps作为开始序号，然后从各个序号后面数 <spanclass="math inline">\(l\)</span> 个steps作为要mask的区域。</p><p>假设<span class="math inline">\(M \subset [T]\)</span>为要mask的区域， <span class="math inline">\(\tilde{X}\)</span>表示被mask之后的输入，定义在mask区域上的entropy-loss为：</p><p><span class="math display">\[L_m(f;X,M,Z)= \sum_{t\in M}logp_f(z_t|\tilde{X},t)\]</span></p><p>相应的定义不在mask区域上的enrtopy-loss：</p><p><span class="math display">\[L_u(f;X,M,Z)= \sum_{t\notin M}logp_f(z_t|\tilde{X},t)\]</span></p><p>最终的loss为：</p><p><span class="math display">\[L = \alpha L_m + (1-\alpha)L_u\]</span></p><p>loss公式中，<span class="math inline">\(p_f\)</span>的公式如下；</p><p><span class="math display">\[p_f^{(k)}(c | \tilde{X}, t) = \frac{\exp(\text{sim}(A^{(k)} o_t,e_c)/\tau )}{\sum\limits_{c&#39; =1}^{C} \exp(\text{sim}(A^{(k)} o_t,e_{c&#39;})/\tau )}\]</span></p><p>其中 <span class="math inline">\(A^{(k)}\)</span>是第k个聚类的投影矩阵，乘以 <span class="math inline">\(o_t\)</span>之后变成C-class的离散变量，其中 <span class="math inline">\([o_1, ...,o_T]\)</span> 为Bert的输出。 <span class="math inline">\(e_c\)</span> 是<span class="math inline">\(c\)</span> 的嵌入向量，<spanclass="math inline">\(sim(·,·)\)</span>是两个向量之间的余弦相似度。<span class="math inline">\(\tau\)</span>是logit scaling factor。</p><p>注意这几个变量分别代表什么。我们可以从两条路径来分析</p><ol type="1"><li><p><strong>音频通过Acoustic Unit Discovery模型，得到codewords <spanclass="math inline">\([z_1,..,z_T]\)</span>，这是一组离散的类别。每一个类别<span class="math inline">\(c\)</span> 我们赋予一个嵌入向量，这就是<spanclass="math inline">\(e_c\)</span>，这是标签生成的路径。</strong></p></li><li><p><strong>音频通过HuBert之后得到 <span class="math inline">\([o_1,..., o_T]\)</span>，乘以投影矩阵 <spanclass="math inline">\(A\)</span>，计算和标签的相似度。</strong></p></li></ol><h2 id="cluster-ensembles">Cluster Ensembles</h2><p>k-means根据初始值和k值的不同，其表现也会大不相同。因此作者采用多个不同参数的k-means进行学习，这就是ClusterEnsembles。设用 <spanclass="math inline">\(Z^{(k)}\)</span>表示第k个聚类模型生成的sequence。故此时：</p><p><span class="math display">\[L_m(f;X,\{Z^{(k)}\}_k,M)=\sum_{t\in M} \sum_{k} logp_f^{k}(z_t^{k}|\tilde{X},t)\]</span></p><h2 id="iterative-refinement-of-cluster-assignments">IterativeRefinement of Cluster Assignments</h2><p>虽然可以直接在MFCCs这样的acousticfeatures上直接得到label，但随着学习的进行，这种label显得太过粗糙。因此，在学习进行到一定程度时，我们在模型学习到的表示上重新学习聚类模型来替代之前的聚类模型，提高了模型的精度。</p>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zipformer论文阅读</title>
    <link href="/2025/02/19/Zipformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/02/19/Zipformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="简介">简介</h1><p>ASR模型，基于Conformer改进</p><h1 id="改进点">改进点</h1><h2 id="downsampled-encoder-structure">Downsampled encoderstructure</h2><ul><li>不同于 Conformer 只在一个固定的帧率 25Hz 操作，Zipformer采用了一个类似于 U-Net的结构，在不同帧率上学习不同时间分辨率的时域表征。</li></ul><h2 id="zipformer-block">Zipformer block</h2><p>扩展conformer块结构</p><h3 id="non-linear-attention">Non-Linear Attention</h3><h3 id="bypass">Bypass</h3><h2 id="biasnorm">BiasNorm</h2><h2 id="swoosh-激活函数">Swoosh 激活函数</h2>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VietASR论文阅读</title>
    <link href="/2025/02/19/VietASR%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/02/19/VietASR%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>VietASR是一个针对小语种的ASRPipeline，结合了HuBert和Zipformer，<strong>本质是HuBert对ASR的适配</strong>。</p><h1 id="对hubert的改动">对HuBert的改动</h1><ol type="1"><li>VietASR使用<strong>Fbank</strong>直接进行掩码，替代音频+CNN。</li></ol><p><img src="/images/HuBert_1.png" alt="HuBert示意图" width="300"></p><ol start="2" type="1"><li>修改损失函数</li></ol><p>回顾，HuBert中</p><p><span class="math display">\[L_m(f;X,M,Z)= \sum_{t\in M}logp_f(z_t|\tilde{X},t)\]</span></p><p><span class="math display">\[p_f^{(k)}(c | \tilde{X}, t) = \frac{\exp(\text{sim}(A^{(k)} o_t,e_c)/\tau )}{\sum\limits_{c&#39; =1}^{C} \exp(\text{sim}(A^{(k)} o_t,e_{c&#39;})/\tau )}\]</span></p><details><summary>一些符号说明</summary><ul><li><p>$ X = [x_1,..,x_T] $ 为T个frame的语音输入。</p></li><li><p>$ Z=[z_1,..,z_T] 为 $Acoustic Unit Discovery模型输出的hiddenunits。</p></li><li><p><span class="math inline">\(z_t \in [C]\)</span>是一个C-class的变量，为语音的离散表示。</p></li><li><p><spanclass="math inline">\(h\)</span>是一个非监督的聚类模型，如k-means onMFCC。</p></li><li><p><spanclass="math inline">\(f\)</span>是HuBert预训练模型。</p></li><li><p><span class="math inline">\(A^{(k)}\)</span>是第k个聚类的投影矩阵，乘以 <span class="math inline">\(o_t\)</span>之后变成C-class的离散变量。</p></li><li><p><span class="math inline">\([o_1, ..., o_T]\)</span>为Bert的输出。</p></li><li><p><span class="math inline">\(e_c\)</span> 是 <spanclass="math inline">\(c\)</span> 的嵌入向量。</p></li><li><p><span class="math inline">\(sim(·,·)\)</span>是两个向量之间的余弦相似度。</p></li><li><p><span class="math inline">\(\tau\)</span> 是logit scalingfactor。</p></li></ul></details><p>而VietASR<strong>修改了损失函数，从余弦相似度变为</strong>（论文中这里<span class="math inline">\(f\)</span> 是loss函数。<spanclass="math inline">\(z\)</span>也不是类别，而是Bert预测的所有类别的softmax概率。这两个记号和HuBert中不同）：</p><p><span class="math display">\[f(\tilde{X}, c) = - \sum_{t \in \text{masked}} \log\frac{\exp(z_{tc_t})}{\sum_{i=1}^C \exp(z_{t,i})} \tag{1}\]</span></p><p><span class="math display">\[o = \text{encoder}(\tilde{X}) = (o_1, o_2, \dots, o_T) \tag{2}\]</span></p><p><span class="math display">\[z_t = A o_t / \tau = (z_{t1}, z_{t2}, \dots, z_{tC}) \tag{3}\]</span></p><p>其中，<span class="math inline">\(X_e\)</span>表示带掩码的输入，<span class="math inline">\(c\)</span>表示簇（cluster）标签，<span class="math inline">\(A\)</span>表示投影矩阵，<span class="math inline">\(\tau\)</span> 为温度参数。</p><p>这种简化方式来自于李宏毅组的工作<ahref="https://arxiv.org/pdf/2211.09944">MELHUBERT: A SIMPLIFIED HUBERTON MELSPECTROGRAMS</a>，优化了cos带来的大量运算，其本质是<strong>使用生成学习而非原来的对比学习</strong>，示意图如下。左侧为HuBert原始结构，右侧为改进后的结构：</p><p><img src="/images/VietASR_loss.png" alt="VietASR与HuBert区别" width="400"></p><h1 id="pipeline">Pipeline</h1><ol type="1"><li><p>在小规模有监督数据上训练一个Zipformer ASR模型，使用RNN-Tloss。</p></li><li><p>对训练好的Zipformer的encoder输出应用k-means，作为HuBert标签。</p></li><li><p>使用上面的标签进行HuBert训练，采用cross-entropy优化。</p></li><li><p>使用pruned RNN-T loss对预训练的encoder微调。</p></li></ol><p><img src="/images/VietASR.png" alt="VietASR Pipeline示意图" width="600"></p>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阿拉伯语ASR</title>
    <link href="/2025/01/20/%E9%98%BF%E6%8B%89%E4%BC%AF%E8%AF%ADASR/"/>
    <url>/2025/01/20/%E9%98%BF%E6%8B%89%E4%BC%AF%E8%AF%ADASR/</url>
    
    <content type="html"><![CDATA[<p>挑战：</p><ul><li>Language Variants:Modern Standard Arabic(MSA), ClassicalArabic(CA，古兰经), and Dialectal Arabic(DA，方言)</li><li>Dialectal Variations：不同地区的方言</li><li>Code-Switching (CS) ：夹杂法文和英文<br /></li><li>Non-Standardized Orthography: DA没有一个uniform orthography</li><li>Annotated Data：缺少标注数据</li></ul>]]></content>
    
    
    <categories>
      
      <category>语音技术</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>阿拉伯语数据集</title>
    <link href="/2025/01/19/%E9%98%BF%E6%8B%89%E4%BC%AF%E8%AF%AD%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <url>/2025/01/19/%E9%98%BF%E6%8B%89%E4%BC%AF%E8%AF%AD%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    
    <content type="html"><![CDATA[<h1 id="casablanca">Casablanca</h1><p>论文：https://anthropology/2024.emnlp-main.1211.pdf</p><p>优势：全人工标注（最大的全监督阿拉伯方言数据集）</p><ul><li><p>时长：48h</p></li><li><p>方言：8种（ALG, EGY, JOR, MOR, UAE, PAL, MAU,YEM），8个标签</p></li><li><p>Segmentation：全人工标注</p></li><li><p>Transcription：全人工标注</p></li><li><p>code-switching ：英文、法文（加音译）</p></li></ul><h1 id="qasr">QASR</h1><p>论文：https://arxiv.org/abs/2106.13000</p><ul><li><p>时长：2000h 新闻频道</p></li><li><p>lightly supervisedtranscriptions（训练集中文本为转录的文本，不完全准确，并非人工标注）</p></li><li><p>采样率：16kHz</p></li><li><p>包含语言学驱动的分段（linguistically motivatedsegmentation）、标点符号（punctuation）、说话者信息（speaker informationamong others）</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>语音技术</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>语音基础知识</title>
    <link href="/2025/01/19/%E8%AF%AD%E9%9F%B3%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <url>/2025/01/19/%E8%AF%AD%E9%9F%B3%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<h1 id="las">LAS</h1><p>Listen: Encoder Attend: Attention Spell:Decoder 典型的seq2seq withattention</p><h2 id="encoder">Encoder</h2><ul><li>输入acoustic features， 输出提取出的特征</li></ul><p><img src="/images/LAS_Listen.png" alt="Listen示意图" width="600"></p><ul><li><p>Encoder可以是RNN+CNN，也可以是self-attention</p></li><li><p>可以做down sampling 降低运算量</p><ul><li>方法：空洞卷积（CNN）、truncated attention</li></ul></li></ul><h2 id="attention">Attention</h2><h2 id="decoder">Decoder</h2><p>可以采用RNN。每一个token输出一个分布，维度=vocabulary_size</p><p><img src="/images/LAS_Speak.png" alt="Spell示意图" width="600"></p><h1 id="ctc">CTC</h1><ul><li><p>CTC可以做到一边听一边辨识，只需要encoder把 <spanclass="math inline">\(h\)</span> 输出，通过一个线性分类器，</p></li><li><p>在CTC中加入了一个特别的token，用 <spanclass="math inline">\(\emptyset\)</span>表示，这是为了对齐语音和文本。在两个 <spanclass="math inline">\(\emptyset\)</span> 中间重复的token合并，并去除<span class="math inline">\(\emptyset\)</span>。</p></li><li><p>训练时，穷举所有可能性。</p></li></ul><p><img src="/images/CTC.jpg" alt="CTC示意图" width="600"></p><h1 id="rna">RNA</h1><p>CTC 的 decoder看不到前面的信息，因此把linearclassifier换成RNN或者LSTM。</p><p><img src="/images/RNA.png" alt="RNA示意图" width="600"></p><h1 id="rnn-t">RNN-T</h1><ul><li><p>吃一个输入，输出多个token</p></li><li><p>看到一个输入之后就一直输出，直到输出到模型觉得满意为止(输出 <spanclass="math inline">\(\emptyset\)</span>)</p></li></ul><p><img src="/images/RNN-T.png" alt="RNN-T" width="600"></p><p>Whisper：弱监督方法。将数据集从4h做到了68h。模型仅仅是Transformer模型。</p>]]></content>
    
    
    <categories>
      
      <category>语音技术</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/01/17/hello-world/"/>
    <url>/2025/01/17/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>补码再思考</title>
    <link href="/2024/01/18/complement/"/>
    <url>/2024/01/18/complement/</url>
    
    <content type="html"><![CDATA[<p>  我曾在很多课程中学习过补码的知识。然而，大多数时候我受到的教育都是机械地接受:<strong>正数和0的补码就是该数字本身再补上最高比特0。负数的补码则是将其绝对值按位取反再加1。补码可以避免+0和-0的歧义。</strong>但是为什么要这么定义，以及它是如何避免+0和-0的，很多地方都没有解释。本文通过另一种看待补码的角度，尝试解释上述问题。</p>  首先，我们回顾原码的定义：<center><strong>原码是指一个二进制数左边加上符号位后所得到的码</strong></center><p>  由于原码是强行添加的符号位，因此<strong>符号位本身并没有权重</strong>。然而，数学知识告诉我们，在一个正常的进制中，每个位上都应该有某个权重。因此，原码在数学上是<strong>不完备的</strong>，势必会产生一些问题。而这个问题在这里就体现为+0和-0。</p><p>  那么事情就变得很清晰了：我们需要给符号位也赋予一个权重。</p><p>  我们采用如下定义：将第N位的权重定义为<spanclass="math inline">\(-2^{(N-1)}\)</span>,第i (i&lt;N)位的权重定义为<span class="math inline">\(-2^{(i-1)}\)</span>。</p><p>  好了，这样我们的符号位也有了权重。但是，为什么是这个权重？我们可以来看看这样定义会发生什么。</p><p>  我们以一个四位二进制数1101为例。当它作为无符号普通二进制数时，1101=13。当它作为补码时，1101=-3。现在我断言，它们在进行运算的时候，作用是相同的。为什么呢？这是因为四位二进制补码的表示范围是<spanclass="math inline">\([-2^{(4-1)},2^{(4-1)-1}]=[-8,7]\)</span>,这是一个长度为16的区间，也就是说，由于溢出的原因，所有得到的结果都要<strong>模16</strong>到[-8,7]这个区间中，所以-3就等于+13。这样，补码就巧妙地将带符号的二进制数中的<strong>负数转化为了正数</strong>，从而可以正常的进行二进制计算，而且更加符合加法器的原理。</p><p>  现在我们来看看0的问题。让我们来看看1101加几等于0。由于模了16，所以16=0。所以1101+0011=10000=0000。而这个0011，也就是1101的相反数。这就是本文最开始的经验性定义的由来——因为我们要凑10000，所以把正数取反相加，就是1111，再加1，就可以完美产生10000这个数。因此，一个负数的补码，就是它对应的正数取反加1。</p><p>  看到这里，你应该知道了补码的本质优势————可以通过模运算的性质，简单地将加法运算扩展到负数，而不需要特殊的硬件电路或逻辑。</p><p>  最后，我还很好奇，为什么反码的英文是“1'scomplement”，补码的英文是“2'scomplement”。关于这方面的中文资料很少。知乎用户 <span class="citation"data-cites="前端西瓜哥">@前端西瓜哥</span> 在他的知乎文章<ahref="https://zhuanlan.zhihu.com/p/498147694">补码到底是什么？</a>中认为： &gt;补码原意是“2的补数”。这个命名虽然没有描述补码的定义，但它描述了补码的一个特性：一个补码可以通过被<spanclass="math inline">\(2^w\)</span>减去，得到它的相反数，即<spanclass="math inline">\(-x=2^w-x\)</span>。</p><p>  我认为这种说法有待商榷，因为以此类推，因为反码是1'scomplement，所以一个反码可以通过被1减去，得到它的相反数。而这显然与反码的定义相冲突。</p><p>  还有一种说法是ChatGPT说的，它说反码只做了一个操作，涉及到1个值，所以是1'scomplement。补码因为还要+1，涉及到2个值，所以是2'scomplement。我认为这种说法也很牵强。总之我没有在中文互联网查到让我满意的答案。欢迎读者查阅相关英文文献并和我交流！</p>]]></content>
    
    
    <categories>
      
      <category>计算机基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
