<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>BPE 论文阅读</title>
    <link href="/2025/03/03/BPE-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/03/03/BPE-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>为了解决罕见词不在词汇表中的问题。</p><p>每次合并出现频率最高的词组。</p><p>唯一超参数为合并次数</p><h1 id="伪代码">伪代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python">Initialize vocabulary <span class="hljs-keyword">with</span> word frequencies<br><br><span class="hljs-type">Set</span> number of merge operations<br><br>Define function get_stats(vocabulary):<br>    Create empty dictionary <span class="hljs-keyword">for</span> pairs<br>    For each word <span class="hljs-keyword">and</span> its frequency <span class="hljs-keyword">in</span> vocabulary:<br>        Split word into symbols<br>        For each adjacent pair of symbols:<br>            Count occurrences of each pair<br>    Return the dictionary of pairs <span class="hljs-keyword">with</span> their frequencies<br><br>Define function merge_vocab(pair, vocabulary):<br>    Create empty dictionary <span class="hljs-keyword">for</span> new vocabulary<br>    Define pattern <span class="hljs-keyword">as</span> the pair joined by space<br>    Define replacement <span class="hljs-keyword">as</span> the pair joined without space<br>    <br>    For each word <span class="hljs-keyword">and</span> its frequency <span class="hljs-keyword">in</span> vocabulary:<br>        Replace occurrences of the pair <span class="hljs-keyword">in</span> the word <span class="hljs-keyword">with</span> the merged version<br>        Store the new word <span class="hljs-keyword">and</span> its frequency <span class="hljs-keyword">in</span> new vocabulary<br>    <br>    Return new vocabulary<br><br>Repeat <span class="hljs-keyword">for</span> the defined number of merge operations:<br>    Get symbol pair frequencies <span class="hljs-keyword">from</span> vocabulary<br>    If no pairs exist, exit loop<br>    Find the most frequent pair<br>    Merge the most frequent pair <span class="hljs-keyword">in</span> vocabulary<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN-Transducer with stateless prediction network 论文阅读</title>
    <link href="/2025/03/03/RNN-Transducer-with-stateless-prediction-network-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/03/03/RNN-Transducer-with-stateless-prediction-network-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>过去， predictionnetwork被认为是语言模型。这篇文章推翻了这一论点。</p><p>证据是，经过实验，一个stateless(例如非recurrent)的predictionnetwork（这种网络仅依赖于上一个时刻输出）在使用wordpieces时，几乎能够与原始RNNT表现相当。</p><p>结论：RNNT 的 prediction network并不像经典 ASR系统中的语言模型那样工作。相反，它的作用仅限于帮助模型 align输入音频，而 RNNT 的encoder 和 joint networksacoustic和linguistic信息。</p><h1 id="实验">实验</h1><p>三组实验：</p><p>首先用文本数据预训练prediction network</p><ol type="1"><li>不冻结</li><li>冻结prediction network</li><li>prediction network中增加两层，冻结</li></ol><figure><img src="/images/stateless_results_1.png"alt="freezing predictor network的WER最高" /><figcaption aria-hidden="true">freezing predictornetwork的WER最高</figcaption></figure><p>结果：freezing predictornetwork的WER最高。这表明文本数据中学到的知识反而会增加WER。</p><h1 id="stateless-prediction-network-的表现">Stateless predictionnetwork 的表现</h1><figure><img src="/images/RNNT-SLP.png" alt="RNNT-SLP结构图" /><figcaption aria-hidden="true">RNNT-SLP结构图</figcaption></figure><p>stateless prediction network 如果使用graphemes会出现重复的问题。由于只看到前一个单词，所以没有信息知道已经说了几个。例如，food可能会输出fooooood。如果使用wordpiece就不会有这个问题。</p>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>unispeech 论文阅读</title>
    <link href="/2025/02/27/unispeech-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/02/27/unispeech-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>利用L、M、N三个数据集（定义见下）。</p><ol type="1"><li><p>在L、M上预训练模型。</p></li><li><p>冻结特征提取器，并在N上对Transformer部分进行微调。</p></li></ol><h1 id="定义">定义</h1><p>L：大规模、有标签的high-resource语种数据集</p><p>M：大规模、无标签的low-resource语种数据集</p><p>N：小规模、有标签的low-resource语种数据集</p><h1 id="模型结构">模型结构</h1><p><img src="/images/unispeech.png" alt="unispeech" width="600"></p><p>使用multitask学习方法。</p><ul><li>目标1：sequence-level CTC loss</li></ul><p><span class="math display">\[L_{\text{ctc}} = - \log p_{\text{ctc}}(y|x) = \sum_{\pi \in \Phi_{x,y}}p(\pi | c_1, \dots, c_T)\]</span></p><ul><li>目标2：基于wav2vec2.0，在masked contextual representations和discretelatent representations之间的对比学习</li></ul><p>对比学习loss:</p><p><span class="math display">\[L_c = - \log \frac{\exp(\text{sim}(c_t, q_t) / \kappa)}{\sum_{q&#39;\sim Q_t} \exp(\text{sim}(c_t, q&#39;) / \kappa)}\]</span></p><p>codebook diversity loss：</p><p><span class="math display">\[\quad L_d = \frac{1}{GV} \sum_{g=1}^{G} \sum_{v=1}^{V} \bar{p}_{g,v}\log \bar{p}_{g,v}\]</span></p><p><span class="math display">\[L_{\text{self}} = L_c + 0.1 \times L_d\]</span></p><p>最终预训练的loss：</p><p><span class="math display">\[\quad L = \sum_{(x,y) \in L} \left( \alpha L_{\text{ctc}} + (1 - \alpha)L_{\text{self}} \right) + \sum_{(x) \in M} L_{\text{self}}\]</span></p><h1 id="问题">问题</h1><ol type="1"><li>训练时是混合训练吗？</li><li>为什么要训练码本？如果码本和Transformer结构要尽量相似的话</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Factorized RNN-T 论文阅读</title>
    <link href="/2025/02/23/factorized_rnnt-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/02/23/factorized_rnnt-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="为什么-neural-transducer-中的predictor无法被视作语言模型">为什么Neural Transducer 中的Predictor无法被视作语言模型</h1><p>唯一区别在于因为需要预测 φ。因此想到可以将Predictor拆分。</p><h1 id="factorized-neural-transducer">Factorized Neural Transducer</h1><p><img src="/images/factorized_neural_Transducer.png" alt="factorized neural Transducer" width="600"></p><p>把Predictor拆分成两部分。</p><ol type="1"><li>专门用于预测空白符号 φ，称为blank predictor;</li><li>另一个用于预测标签词汇（不包括 φ）， 称为vocabulary predictor</li></ol><p>这样，Predictor_v就相当于一个语言模型。</p><h1 id="loss-function">Loss function</h1><p>原始的Transducer的Loss为：</p><p><span class="math display">\[ J_t = -\log P(y \in Y^* | x) = -\log\sum_{\alpha \in \beta^{-1}(y)} P(\alpha | x) \]</span></p><p>而factorized_rnnt的Loss为：</p><p><span class="math display">\[J_f = J_t - \lambda \log P(y_1^U)\]</span></p>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HuBert论文阅读</title>
    <link href="/2025/02/19/HuBert%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/02/19/HuBert%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<figure><img src="/images/HuBert_1.png" alt="HuBert示意图" /><figcaption aria-hidden="true">HuBert示意图</figcaption></figure><h1 id="概述">概述</h1><ul><li>作用：从无监督数据中学习隐层表示。</li><li>端口：输入为音频，输出为隐层表示。</li></ul><h1 id="训练过程">训练过程：</h1><ol type="1"><li>利用Acoustic Unit Discovery System得到label</li><li>通过HuBERT从mask之后的输入中得到隐层表示z</li><li>计算label和预测之间的loss</li></ol><h1 id="细节">细节</h1><h2 id="acoustic-unit-discovery">Acoustic Unit Discovery</h2><p>可以理解为语音离散化。</p><p>一些记号：</p><p>T个frame的语音 $ X = [x_1,..,x_T] $。Acoustic UnitDiscovery模型输出的hidden units为 $ h(X)=Z=[z_1,..,z_T]$，其中 <spanclass="math inline">\(z_t \in [C]\)</span>是一个C-class的变量。h是一个非监督的聚类模型，如k-means on MFCC。</p><h2 id="mask">Mask</h2><p>使用了和wav2vec 2类似的方法，首先选择 <spanclass="math inline">\(p%\)</span>time-steps作为开始序号，然后从各个序号后面数 <spanclass="math inline">\(l\)</span> 个steps作为要mask的区域。</p><p>假设<span class="math inline">\(M \subset [T]\)</span>为要mask的区域， <span class="math inline">\(\tilde{X}\)</span>表示被mask之后的输入，定义在mask区域上的entropy-loss为：</p><p><span class="math display">\[L_m(f;X,M,Z)= \sum_{t\in M}logp_f(z_t|\tilde{X},t)\]</span></p><p>相应的定义不在mask区域上的enrtopy-loss：</p><p><span class="math display">\[L_u(f;X,M,Z)= \sum_{t\notin M}logp_f(z_t|\tilde{X},t)\]</span></p><p>最终的loss为：</p><p><span class="math display">\[L = \alpha L_m + (1-\alpha)L_u\]</span></p><p>loss公式中，<span class="math inline">\(p_f\)</span>的公式如下；</p><p><span class="math display">\[p_f^{(k)}(c | \tilde{X}, t) = \frac{\exp(\text{sim}(A^{(k)} o_t,e_c)/\tau )}{\sum\limits_{c&#39; =1}^{C} \exp(\text{sim}(A^{(k)} o_t,e_{c&#39;})/\tau )}\]</span></p><p>其中 <span class="math inline">\(A^{(k)}\)</span>是第k个聚类的投影矩阵，乘以 <span class="math inline">\(o_t\)</span>之后变成C-class的离散变量，其中 <span class="math inline">\([o_1, ...,o_T]\)</span> 为Bert的输出。 <span class="math inline">\(e_c\)</span> 是<span class="math inline">\(c\)</span> 的嵌入向量，<spanclass="math inline">\(sim(·,·)\)</span>是两个向量之间的余弦相似度。<span class="math inline">\(\tau\)</span>是logit scaling factor。</p><p>注意这几个变量分别代表什么。我们可以从两条路径来分析</p><ol type="1"><li><p><strong>音频通过Acoustic Unit Discovery模型，得到codewords <spanclass="math inline">\([z_1,..,z_T]\)</span>，这是一组离散的类别。每一个类别<span class="math inline">\(c\)</span> 我们赋予一个嵌入向量，这就是<spanclass="math inline">\(e_c\)</span>，这是标签生成的路径。</strong></p></li><li><p><strong>音频通过HuBert之后得到 <span class="math inline">\([o_1,..., o_T]\)</span>，乘以投影矩阵 <spanclass="math inline">\(A\)</span>，计算和标签的相似度。</strong></p></li></ol><h2 id="cluster-ensembles">Cluster Ensembles</h2><p>k-means根据初始值和k值的不同，其表现也会大不相同。因此作者采用多个不同参数的k-means进行学习，这就是ClusterEnsembles。设用 <spanclass="math inline">\(Z^{(k)}\)</span>表示第k个聚类模型生成的sequence。故此时：</p><p><span class="math display">\[L_m(f;X,\{Z^{(k)}\}_k,M)=\sum_{t\in M} \sum_{k} logp_f^{k}(z_t^{k}|\tilde{X},t)\]</span></p><h2 id="iterative-refinement-of-cluster-assignments">IterativeRefinement of Cluster Assignments</h2><p>虽然可以直接在MFCCs这样的acousticfeatures上直接得到label，但随着学习的进行，这种label显得太过粗糙。因此，在学习进行到一定程度时，我们在模型学习到的表示上重新学习聚类模型来替代之前的聚类模型，提高了模型的精度。</p>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zipformer论文阅读</title>
    <link href="/2025/02/19/Zipformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/02/19/Zipformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="简介">简介</h1><p>ASR模型，基于Conformer改进</p><h1 id="改进点">改进点</h1><h2 id="downsampled-encoder-structure">Downsampled encoderstructure</h2><ul><li>不同于 Conformer 只在一个固定的帧率 25Hz 操作，Zipformer采用了一个类似于 U-Net的结构，在不同帧率上学习不同时间分辨率的时域表征。</li></ul><h2 id="zipformer-block">Zipformer block</h2><p>扩展conformer块结构</p><h3 id="non-linear-attention">Non-Linear Attention</h3><h3 id="bypass">Bypass</h3><h2 id="biasnorm">BiasNorm</h2><h2 id="swoosh-激活函数">Swoosh 激活函数</h2>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VietASR论文阅读</title>
    <link href="/2025/02/19/VietASR%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/02/19/VietASR%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>VietASR是一个针对小语种的ASRPipeline，结合了HuBert和Zipformer，<strong>本质是HuBert对ASR的适配</strong>。</p><h1 id="对hubert的改动">对HuBert的改动</h1><ol type="1"><li>VietASR使用<strong>Fbank</strong>直接进行掩码，替代音频+CNN。</li></ol><p><img src="/images/HuBert_1.png" alt="HuBert示意图" width="300"></p><ol start="2" type="1"><li>修改损失函数</li></ol><p>回顾，HuBert中</p><p><span class="math display">\[L_m(f;X,M,Z)= \sum_{t\in M}logp_f(z_t|\tilde{X},t)\]</span></p><p><span class="math display">\[p_f^{(k)}(c | \tilde{X}, t) = \frac{\exp(\text{sim}(A^{(k)} o_t,e_c)/\tau )}{\sum\limits_{c&#39; =1}^{C} \exp(\text{sim}(A^{(k)} o_t,e_{c&#39;})/\tau )}\]</span></p><details><summary>一些符号说明</summary><ul><li><p>$ X = [x_1,..,x_T] $ 为T个frame的语音输入。</p></li><li><p>$ Z=[z_1,..,z_T] 为 $Acoustic Unit Discovery模型输出的hiddenunits。</p></li><li><p><span class="math inline">\(z_t \in [C]\)</span>是一个C-class的变量，为语音的离散表示。</p></li><li><p><spanclass="math inline">\(h\)</span>是一个非监督的聚类模型，如k-means onMFCC。</p></li><li><p><spanclass="math inline">\(f\)</span>是HuBert预训练模型。</p></li><li><p><span class="math inline">\(A^{(k)}\)</span>是第k个聚类的投影矩阵，乘以 <span class="math inline">\(o_t\)</span>之后变成C-class的离散变量。</p></li><li><p><span class="math inline">\([o_1, ..., o_T]\)</span>为Bert的输出。</p></li><li><p><span class="math inline">\(e_c\)</span> 是 <spanclass="math inline">\(c\)</span> 的嵌入向量。</p></li><li><p><span class="math inline">\(sim(·,·)\)</span>是两个向量之间的余弦相似度。</p></li><li><p><span class="math inline">\(\tau\)</span> 是logit scalingfactor。</p></li></ul></details><p>而VietASR<strong>修改了损失函数，从余弦相似度变为</strong>（论文中这里<span class="math inline">\(f\)</span> 是loss函数。<spanclass="math inline">\(z\)</span>也不是类别，而是Bert预测的所有类别的softmax概率。这两个记号和HuBert中不同）：</p><p><span class="math display">\[f(\tilde{X}, c) = - \sum_{t \in \text{masked}} \log\frac{\exp(z_{tc_t})}{\sum_{i=1}^C \exp(z_{t,i})} \tag{1}\]</span></p><p><span class="math display">\[o = \text{encoder}(\tilde{X}) = (o_1, o_2, \dots, o_T) \tag{2}\]</span></p><p><span class="math display">\[z_t = A o_t / \tau = (z_{t1}, z_{t2}, \dots, z_{tC}) \tag{3}\]</span></p><p>其中，<span class="math inline">\(X_e\)</span>表示带掩码的输入，<span class="math inline">\(c\)</span>表示簇（cluster）标签，<span class="math inline">\(A\)</span>表示投影矩阵，<span class="math inline">\(\tau\)</span> 为温度参数。</p><p>这种简化方式来自于李宏毅组的工作<ahref="https://arxiv.org/pdf/2211.09944">MELHUBERT: A SIMPLIFIED HUBERTON MELSPECTROGRAMS</a>，优化了cos带来的大量运算，其本质是<strong>使用生成学习而非原来的对比学习</strong>，示意图如下。左侧为HuBert原始结构，右侧为改进后的结构：</p><p><img src="/images/VietASR_loss.png" alt="VietASR与HuBert区别" width="400"></p><h1 id="pipeline">Pipeline</h1><ol type="1"><li><p>在小规模有监督数据上训练一个Zipformer ASR模型，使用RNN-Tloss。</p></li><li><p>对训练好的Zipformer的encoder输出应用k-means，作为HuBert标签。</p></li><li><p>使用上面的标签进行HuBert训练，采用cross-entropy优化。</p></li><li><p>使用pruned RNN-T loss对预训练的encoder微调。</p></li></ol><p><img src="/images/VietASR.png" alt="VietASR Pipeline示意图" width="600"></p>]]></content>
    
    
    
    <tags>
      
      <tag>语音技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阿拉伯语ASR</title>
    <link href="/2025/01/20/%E9%98%BF%E6%8B%89%E4%BC%AF%E8%AF%ADASR/"/>
    <url>/2025/01/20/%E9%98%BF%E6%8B%89%E4%BC%AF%E8%AF%ADASR/</url>
    
    <content type="html"><![CDATA[<p>挑战：</p><ul><li>Language Variants:Modern Standard Arabic(MSA), ClassicalArabic(CA，古兰经), and Dialectal Arabic(DA，方言)</li><li>Dialectal Variations：不同地区的方言</li><li>Code-Switching (CS) ：夹杂法文和英文<br /></li><li>Non-Standardized Orthography: DA没有一个uniform orthography</li><li>Annotated Data：缺少标注数据</li></ul>]]></content>
    
    
    <categories>
      
      <category>语音技术</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>阿拉伯语数据集</title>
    <link href="/2025/01/19/%E9%98%BF%E6%8B%89%E4%BC%AF%E8%AF%AD%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <url>/2025/01/19/%E9%98%BF%E6%8B%89%E4%BC%AF%E8%AF%AD%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    
    <content type="html"><![CDATA[<h1 id="casablanca">Casablanca</h1><p>论文：https://anthropology/2024.emnlp-main.1211.pdf</p><p>优势：全人工标注（最大的全监督阿拉伯方言数据集）</p><ul><li><p>时长：48h</p></li><li><p>方言：8种（ALG, EGY, JOR, MOR, UAE, PAL, MAU,YEM），8个标签</p></li><li><p>Segmentation：全人工标注</p></li><li><p>Transcription：全人工标注</p></li><li><p>code-switching ：英文、法文（加音译）</p></li></ul><h1 id="qasr">QASR</h1><p>论文：https://arxiv.org/abs/2106.13000</p><ul><li><p>时长：2000h 新闻频道</p></li><li><p>lightly supervisedtranscriptions（训练集中文本为转录的文本，不完全准确，并非人工标注）</p></li><li><p>采样率：16kHz</p></li><li><p>包含语言学驱动的分段（linguistically motivatedsegmentation）、标点符号（punctuation）、说话者信息（speaker informationamong others）</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>语音技术</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>语音基础知识</title>
    <link href="/2025/01/19/%E8%AF%AD%E9%9F%B3%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <url>/2025/01/19/%E8%AF%AD%E9%9F%B3%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<h1 id="las">LAS</h1><p>Listen: Encoder Attend: Attention Spell:Decoder 典型的seq2seq withattention</p><h2 id="encoder">Encoder</h2><ul><li>输入acoustic features， 输出提取出的特征</li></ul><p><img src="/images/LAS_Listen.png" alt="Listen示意图" width="600"></p><ul><li><p>Encoder可以是RNN+CNN，也可以是self-attention</p></li><li><p>可以做down sampling 降低运算量</p><ul><li>方法：空洞卷积（CNN）、truncated attention</li></ul></li></ul><h2 id="attention">Attention</h2><h2 id="decoder">Decoder</h2><p>可以采用RNN。每一个token输出一个分布，维度=vocabulary_size</p><p><img src="/images/LAS_Speak.png" alt="Spell示意图" width="600"></p><h1 id="ctc">CTC</h1><ul><li><p>CTC可以做到一边听一边辨识，只需要encoder把 <spanclass="math inline">\(h\)</span> 输出，通过一个线性分类器，</p></li><li><p>在CTC中加入了一个特别的token，用 <spanclass="math inline">\(\emptyset\)</span>表示，这是为了对齐语音和文本。在两个 <spanclass="math inline">\(\emptyset\)</span> 中间重复的token合并，并去除<span class="math inline">\(\emptyset\)</span>。</p></li><li><p>训练时，穷举所有可能性。</p></li></ul><p><img src="/images/CTC.jpg" alt="CTC示意图" width="600"></p><h1 id="rna">RNA</h1><p>CTC 的 decoder看不到前面的信息，因此把linearclassifier换成RNN或者LSTM。</p><p><img src="/images/RNA.png" alt="RNA示意图" width="600"></p><h1 id="rnn-t">RNN-T</h1><ul><li><p>吃一个输入，输出多个token</p></li><li><p>看到一个输入之后就一直输出，直到输出到模型觉得满意为止(输出 <spanclass="math inline">\(\emptyset\)</span>)</p></li></ul><p><img src="/images/RNN-T.png" alt="RNN-T" width="600"></p><p>Whisper：弱监督方法。将数据集从4h做到了68h。模型仅仅是Transformer模型。</p>]]></content>
    
    
    <categories>
      
      <category>语音技术</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/01/17/hello-world/"/>
    <url>/2025/01/17/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>补码再思考</title>
    <link href="/2024/01/18/complement/"/>
    <url>/2024/01/18/complement/</url>
    
    <content type="html"><![CDATA[<p>  我曾在很多课程中学习过补码的知识。然而，大多数时候我受到的教育都是机械地接受:<strong>正数和0的补码就是该数字本身再补上最高比特0。负数的补码则是将其绝对值按位取反再加1。补码可以避免+0和-0的歧义。</strong>但是为什么要这么定义，以及它是如何避免+0和-0的，很多地方都没有解释。本文通过另一种看待补码的角度，尝试解释上述问题。</p>  首先，我们回顾原码的定义：<center><strong>原码是指一个二进制数左边加上符号位后所得到的码</strong></center><p>  由于原码是强行添加的符号位，因此<strong>符号位本身并没有权重</strong>。然而，数学知识告诉我们，在一个正常的进制中，每个位上都应该有某个权重。因此，原码在数学上是<strong>不完备的</strong>，势必会产生一些问题。而这个问题在这里就体现为+0和-0。</p><p>  那么事情就变得很清晰了：我们需要给符号位也赋予一个权重。</p><p>  我们采用如下定义：将第N位的权重定义为<spanclass="math inline">\(-2^{(N-1)}\)</span>,第i (i&lt;N)位的权重定义为<span class="math inline">\(-2^{(i-1)}\)</span>。</p><p>  好了，这样我们的符号位也有了权重。但是，为什么是这个权重？我们可以来看看这样定义会发生什么。</p><p>  我们以一个四位二进制数1101为例。当它作为无符号普通二进制数时，1101=13。当它作为补码时，1101=-3。现在我断言，它们在进行运算的时候，作用是相同的。为什么呢？这是因为四位二进制补码的表示范围是<spanclass="math inline">\([-2^{(4-1)},2^{(4-1)-1}]=[-8,7]\)</span>,这是一个长度为16的区间，也就是说，由于溢出的原因，所有得到的结果都要<strong>模16</strong>到[-8,7]这个区间中，所以-3就等于+13。这样，补码就巧妙地将带符号的二进制数中的<strong>负数转化为了正数</strong>，从而可以正常的进行二进制计算，而且更加符合加法器的原理。</p><p>  现在我们来看看0的问题。让我们来看看1101加几等于0。由于模了16，所以16=0。所以1101+0011=10000=0000。而这个0011，也就是1101的相反数。这就是本文最开始的经验性定义的由来——因为我们要凑10000，所以把正数取反相加，就是1111，再加1，就可以完美产生10000这个数。因此，一个负数的补码，就是它对应的正数取反加1。</p><p>  看到这里，你应该知道了补码的本质优势————可以通过模运算的性质，简单地将加法运算扩展到负数，而不需要特殊的硬件电路或逻辑。</p><p>  最后，我还很好奇，为什么反码的英文是“1'scomplement”，补码的英文是“2'scomplement”。关于这方面的中文资料很少。知乎用户 <span class="citation"data-cites="前端西瓜哥">@前端西瓜哥</span> 在他的知乎文章<ahref="https://zhuanlan.zhihu.com/p/498147694">补码到底是什么？</a>中认为： &gt;补码原意是“2的补数”。这个命名虽然没有描述补码的定义，但它描述了补码的一个特性：一个补码可以通过被<spanclass="math inline">\(2^w\)</span>减去，得到它的相反数，即<spanclass="math inline">\(-x=2^w-x\)</span>。</p><p>  我认为这种说法有待商榷，因为以此类推，因为反码是1'scomplement，所以一个反码可以通过被1减去，得到它的相反数。而这显然与反码的定义相冲突。</p><p>  还有一种说法是ChatGPT说的，它说反码只做了一个操作，涉及到1个值，所以是1'scomplement。补码因为还要+1，涉及到2个值，所以是2'scomplement。我认为这种说法也很牵强。总之我没有在中文互联网查到让我满意的答案。欢迎读者查阅相关英文文献并和我交流！</p>]]></content>
    
    
    <categories>
      
      <category>计算机基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
